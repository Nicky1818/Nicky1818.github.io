- ###  简易神经网络代码整体结构
代码分为以下几个部分：

数据生成：生成带有噪声的线性数据。

数据加载：提供一个数据加载器，用于按批次获取数据。

模型定义：定义一个简单的线性模型。

损失函数：定义平均绝对误差（MAE）作为损失函数。

优化器：实现随机梯度下降（SGD）来更新模型参数。

训练过程：通过多轮训练，逐步优化模型参数。

结果可视化：绘制数据和模型的拟合结果。

- ### 1. 导入库

```python
import torch
import matplotlib.pyplot as plt #画图的
import random #随机操作
```
### 2.数据生成函数

**功能**
生成符合线性关系 y = x * w + b 的数据，并添加噪声模拟真实数据。
```python
def creat_data(w, b, data_num):
    x = torch.normal(0, 1, (data_num, len(w)))  # 生成输入数据
    y = torch.matmul(x, w) + b                  # 计算目标值
    noise = torch.normal(0, 0.01, y.shape)      # 生成噪声
    y += noise                                  # 添加噪声
    return x, y
```
**逐行解析：**
1. def creat_data(w, b, data_num):：定义名为 creat_data 的函数，接受三个参数：权重 w、偏置 b、数据量 data_num。
2. x = torch.normal(0, 1, (data_num, len(w)))：生成输入数据 x。torch.normal(均值, 标准差, 形状) 用于生成正态分布随机数。(data_num, len(w)) 表示生成 data_num 行、len(w) 列的矩阵，例如 data_num = 500，len(w) = 4 时，x 是 500x4 的矩阵。
3. y = torch.matmul(x, w) + b：计算目标值 y。torch.matmul(x, w) 是矩阵乘法，将 x 的每一行与 w 做点积，+ b 表示加上偏置项。
4. noise = torch.normal(0, 0.01, y.shape)：生成噪声，形状与 y 相同，均值为 0，标准差为 0.01。
5. y += noise：将噪声添加到 y，模拟真实数据中的随机误差。
6. return x, y：返回生成的输入数据 x 和目标值 y。

### 3. 生成数据
```python
num = 500
true_w = torch.tensor([8.1, 2, 2, 4])
true_b = torch.tensor(1.1)
X, Y = creat_data(true_w, true_b, num)
```
**逐行解析：**
1. num = 500：定义生成数据点的数量为 500。
2. true_w = torch.tensor([8.1, 2, 2, 4])：定义真实的权重向量 w，长度为 4。torch.tensor 用于将列表转换为 PyTorch 张量。
3. true_b = torch.tensor(1.1)：定义真实的偏置项 b，值为 1.1。
4. X, Y = creat_data(true_w, true_b, num)：调用 creat_data 函数生成数据。X 是输入数据，形状为 (500, 4)；Y 是目标值，形状为 (500,)（即长度为 500 的向量）。

### 4. 数据可视化
```python
plt.scatter(X[:, 3], Y, 1)
plt.show()
```
**逐行解析：**
1. plt.scatter(X[:, 3], Y, 1)：绘制散点图，横轴为 X 的第 4 个特征（索引为 3），纵轴为 Y。X[:, 3] 表示取所有行的第 4 列（Python 索引从 0 开始），1 表示点的大小为 1。选择第 4 个特征是因为 true_w 的第 4 个元素是 4，对 Y 的影响最大，可视化效果更明显。
2. plt.show()：显示图形。

### 5. 定义数据加载器
```python
def data_provider(data, label, batch_size):  # 每次访问这个函数，就能提供一批数据
    length = len(label)
    indices = list(range(length))
    random.shuffle(indices)  # 打乱索引

    for each in range(0, length, batch_size):
        get_indices = indices[each: each + batch_size]
        gst_data = data[get_indices]
        get_label = label[get_indices]
        yield gst_data, get_label  # 生成器
```
**逐行解析：**
1. def data_provider(data, label, batch_size):：定义数据加载函数，接受三个参数：输入数据 data、目标值 label、批次大小 batch_size。
2. length = len(label)：获取数据总量（即 label 的长度）。
3. indices = list(range(length))：生成索引列表 [0, 1, 2, ..., length - 1]。
4. random.shuffle(indices)：打乱索引顺序，确保每个批次的数据是随机的。
5. for each in range(0, length, batch_size):：循环遍历数据，每次步长为 batch_size。例如，batch_size = 16 时，循环变量 each 依次为 0, 16, 32, ...。
6. get_indices = indices[each: each + batch_size]：取出当前批次的索引（如 0~15、16~31 等）。
7. gst_data = data[get_indices]：根据索引取出对应的输入数据。
8. get_label = label[get_indices]：根据索引取出对应的目标值。
9. yield gst_data, get_label：使用生成器（yield）返回当前批次的数据，避免一次性加载所有数据到内存。

### 6. 定义模型
```python
def fun(x, w, b):
    pred_y = torch.matmul(x, w) + b
    return pred_y
```
**逐行解析：**
1. def fun(x, w, b):：定义线性模型函数，接受输入 x、权重 w、偏置 b。
2. pred_y = torch.matmul(x, w) + b：计算预测值 pred_y。torch.matmul(x, w) 是矩阵乘法，例如 x 是 16x4 的批次数据，w 是 4x1 的向量，结果 pred_y 是 16x1 的向量。
3. return pred_y：返回预测值。

### 7. 定义损失函数
```python
def maeLoss(pre_y, y):
    return torch.sum(abs(pre_y - y)) / len(y)
```
**逐行解析：**
1. def maeLoss(pre_y, y):：定义平均绝对误差（MAE）损失函数，接受预测值 pre_y 和真实值 y。
2. torch.sum(abs(pre_y - y))：计算所有样本的绝对误差之和。abs(pre_y - y) 表示每个样本的预测值与真实值的绝对误差。
3. / len(y)：求平均值，得到平均绝对误差。

### 8. 定义优化器
```python
def sgd(paras, lr):  # 随机梯度下降，更新参数
    with torch.no_grad():  # 不计算梯度
        for para in paras:
            para -= para.grad * lr
            para.grad.zero_()  # 清空梯度
```
**逐行解析：**
1. def sgd(paras, lr):：定义随机梯度下降（SGD）优化器，接受参数列表 paras 和学习率 lr。
2. with torch.no_grad():：上下文管理器，内部代码不计算梯度（因为参数更新不需要梯度）。
3. for para in paras:：遍历所有参数（如 w 和 b）。
4. para -= para.grad * lr：沿梯度反方向更新参数。para.grad 是参数的梯度，lr 是学习率。
5. para.grad.zero_()：清空梯度，防止梯度累积影响下一轮计算。注意：zero_() 是原地操作（带下划线的函数会修改张量本身）。

### 9. 初始化参数
```python
lr = 0.03
w_0 = torch.normal(0, 0.01, true_w.shape, requires_grad=True)  # 需要计算梯度
b_0 = torch.tensor(0.01, requires_grad=True)
print(w_0, b_0)
```
**逐行解析：**
1. lr = 0.03：定义学习率（learning rate），控制参数更新的步长。
2. w_0 = torch.normal(0, 0.01, true_w.shape, requires_grad=True)：初始化权重 w，形状与 true_w 相同（即 4 维向量）。torch.normal(0, 0.01, ...) 表示从均值为 0、标准差为 0.01 的正态分布采样。requires_grad = True 表示启用梯度计算，PyTorch 会跟踪此张量的所有操作以计算梯度。
3. b_0 = torch.tensor(0.01, requires_grad=True)：初始化偏置 b，初始值为 0.01，并启用梯度计算。
4. print(w_0, b_0)：打印初始化的参数，方便调试。

### 10. 训练循环
```python
epochs = 50  # 训练50轮

for epoch in range(epochs):
    data_loss = 0
    for batch_x, batch_y in data_provider(X, Y, batch_size):
        pred_y = fun(batch_x, w_0, b_0)  # 前向传播
        loss = maeLoss(pred_y, batch_y)  # 计算损失
        loss.backward()                  # 反向传播
        sgd([w_0, b_0], lr)             # 更新参数
        data_loss += loss                # 累计损失
    print("epoch %03d: loss: %.6f" % (epoch, data_loss))
```
**逐行解析：**
1. epochs = 50：定义训练轮次为 50。
2. for epoch in range(epochs):：外层循环，遍历每一轮训练。
3. data_loss = 0：初始化当前轮次的总损失为 0。
4. for batch_x, batch_y in data_provider(X, Y, batch_size):：内层循环，遍历每一个批次的数据。
5. pred_y = fun(batch_x, w_0, b_0)：前向传播，计算当前批次的预测值。
6. loss = maeLoss(pred_y, batch_y)：计算当前批次的损失（平均绝对误差）。
7. loss.backward()：反向传播，自动计算所有 requires_grad = True 的张量的梯度。
8. sgd([w_0, b_0], lr)：调用优化器，更新参数 w 和 b。
9. data_loss += loss：累计当前轮次的总损失。
10. print("epoch %03d: loss: %.6f" % (epoch, data_loss))：打印当前轮次的编号和总损失。%03d 表示用 3 位数字显示轮次（如 001, 002），%.6f 表示保留 6 位小数显示损失。

### 11. 结果输出与可视化
```python
print("真实的函数值是", true_w, true_b)
print("深度训练得到的参数值是", w_0, b_0)

idx = 3
plt.plot(X[:, idx].detach().numpy(), 
         X[:, idx].detach().numpy() * w_0[idx].detach().numpy() + b_0.detach().numpy())
plt.scatter(X[:, idx], Y, 1)
plt.show()
```
**逐行解析：**
1. print("真实的函数值是", true_w, true_b)：打印真实的权重 true_w 和偏置 true_b。
2. print("深度训练得到的参数值是", w_0, b_0)：打印训练后的参数 w_0 和 b_0，对比真实值。
3. idx = 3：选择第 4 个特征（索引为 3）进行可视化。
4. plt.plot(X[:, idx].detach().numpy(), ...)：绘制拟合直线。X[:, idx].detach().numpy() 表示将张量从计算图分离并转换为 NumPy 数组，X[:, idx] * w_0[idx] + b_0 表示根据训练后的参数计算直线上的点。
5. plt.scatter(X[:, idx], Y, 1)：绘制数据点的散点图。
6. plt.show()：显示图形。

### 总结
通过逐行解析，你已经理解了这段代码的每一个细节。接下来可以尝试以下练习：
修改 true_w 和 true_b，观察模型是否能准确拟合。
调整 lr 和 batch_size，观察训练效果的变化。
尝试使用均方误差（MSE）代替 MAE 作为损失函数。